# Luna

Luna is a small transformer-based language model I built when I was 16.
It can handle simple conversational tasks and represents the first full transformer implementation I wrote from scratch.

The version in this repository is a basic, educational implementation.
Its purpose was to help my younger self better understand how language model algorithms work, rather than to achieve high performance or efficiency.
You can train Luna on your own dataset, but it is not designed for large-scale or optimized use.

In a separate folder, I included a more efficient version that I built later, after learning more about model architectures and training algorithms.
Those improved versions reflect the progress I made as I studied transformers in greater depth.

Luna helped me understand the core concepts behind transformers—from attention mechanisms to weight updates—and represents my first step into machine learning.

# Luna

Luna is a small transformer-based language model I built when I was 16.
It can handle simple conversational tasks and represents the first full transformer implementation I wrote from scratch.

This repository contains a basic, educational version of the model.
You can train Luna on your own dataset, but I do not recommend using this version for large-scale or high-efficiency tasks, since it is designed for clarity rather than performance.

In a separate folder, I included a more efficient version that I built later, after learning more about model architectures and training algorithms.
That improved version reflects the progress I made as I studied transformers in greater depth.

Luna was the project that helped me understand how transformers actually learn—from attention to weight updates—and it marks my first step into machine learning.
